{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''MIT License\n",
    "\n",
    "Copyright (c) 2025 Prateek De\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "THIS IS A PROPERTY OF MINDSCRIBE TECH.. PLEASE DO NOT COPY OR DISTRIBUTE WITHOUT PERMISSION\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Neural Network with GloVe Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Breaking the existing dataset into 3 sectors: Train, Validation, Tests\n",
    "data = load_dataset('go_emotions')\n",
    "df_train = pd.DataFrame(data[\"train\"])\n",
    "df_val = pd.DataFrame(data[\"validation\"])\n",
    "df_test = pd.DataFrame(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert lists into multi-label format\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_train['labels_vector'] = mlb.fit_transform(df_train[\"labels\"]).tolist()\n",
    "\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract label mapping\n",
    "label_names = data['train'].features['labels'].feature.names\n",
    "# print(label_names)  # This will give you something like ['admiration', 'amusement', 'anger', ...]\n",
    "# df_train['labels_text'] = df_train['labels'].apply(lambda x: [label_names[i] for i in x])\n",
    "# print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove emails\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    return \" \".join(tokens)  # Return cleaned text\n",
    "\n",
    "# Load dataset\n",
    "df_train['text'] = df_train['text'].fillna(\"\")  # Handle NaN values\n",
    "df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Define local path for GloVe file\n",
    "glove_file_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "# Check if the file already exists locally\n",
    "if not os.path.exists(glove_file_path):\n",
    "    print(\"GloVe file not found. Downloading from Google Drive...\")\n",
    "    file_id = \"1QsPKoMTyODoqTklndJcbUjVdnGQ804H-\"\n",
    "    try:\n",
    "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", glove_file_path, quiet=False)\n",
    "        print(\"Download completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading: {e}\")\n",
    "        print(\"Please download the file manually from Google Drive\")\n",
    "else:\n",
    "    print(\"GloVe file already exists locally.\")\n",
    "\n",
    "# Load GloVe word embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Convert text to GloVe embeddings\n",
    "def preprocess_for_glove(text, embedding_dict, dim=100):\n",
    "    tokens = text.split()  # Tokenize text (already preprocessed)\n",
    "    embeddings = [embedding_dict[word] for word in tokens if word in embedding_dict]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(dim)\n",
    "\n",
    "df_train['embedded_text'] = df_train['clean_text'].apply(lambda text: preprocess_for_glove(text, glove_embeddings))\n",
    "\n",
    "# Add additional features\n",
    "df_train['sentence_length'] = df_train['clean_text'].apply(lambda x: len(x.split()))\n",
    "df_train['uppercase_ratio'] = df_train['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "Neural Network with 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # Multi-label classification needs Sigmoid activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "\n",
    "# Training with K-Fold Cross-Validation\n",
    "def train_evaluate_model(X_tensor, y_tensor, input_size, hidden_size, num_classes, kf, epochs, learning_rate, criterion, optimizer_class, device):\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_tensor):\n",
    "        X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "        y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "\n",
    "        model = SimpleNN(input_size, hidden_size, num_classes).to(device)\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_acc = ((model(X_train) > 0.5) == y_train).float().mean().item()  # Threshold at 0.5 for multi-label\n",
    "            val_acc = ((model(X_val) > 0.5) == y_val).float().mean().item()\n",
    "            training_accuracies.append(train_acc)\n",
    "            validation_accuracies.append(val_acc)\n",
    "\n",
    "    results = {\n",
    "        'Training Accuracy Mean': np.mean(training_accuracies),\n",
    "        'Training Accuracy StdDev': np.std(training_accuracies),\n",
    "        'Validation Accuracy Mean': np.mean(validation_accuracies),\n",
    "        'Validation Accuracy StdDev': np.std(validation_accuracies)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['labels'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Apply preprocessing with word embeddings\n",
    "X_glove = np.array(df_train['embedded_text'].tolist())\n",
    "y_glove = np.array(df_train['labels_vector'].tolist())\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor_glove = torch.tensor(X_glove, dtype=torch.float32)\n",
    "y_tensor_glove = torch.tensor(y_glove, dtype=torch.float32)\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_tensor_glove = X_tensor_glove.to(device)\n",
    "y_tensor_glove = y_tensor_glove.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 100  # GloVe embedding size (100D)\n",
    "hidden_size = 128\n",
    "num_classes = len(label_names)  # Number of emotion labels in GoEmotions dataset\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "kf = KFold(n_splits=5)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Multi-label classification requires BCE loss\n",
    "optimizer_class = optim.Adam\n",
    "\n",
    "# Train the model\n",
    "cv_results = train_evaluate_model(X_tensor_glove, y_tensor_glove, input_size, hidden_size, num_classes, kf, epochs, learning_rate, criterion, optimizer_class, device)\n",
    "\n",
    "# Print Results\n",
    "from tabulate import tabulate\n",
    "print(tabulate([cv_results], headers=\"keys\", tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
