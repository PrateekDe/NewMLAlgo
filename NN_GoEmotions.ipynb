{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Neural Network with GloVe Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Breaking the existing dataset into 3 sectors: Train, Validation, Tests\n",
    "data = load_dataset('go_emotions')\n",
    "df_train = pd.DataFrame(data[\"train\"])\n",
    "df_val = pd.DataFrame(data[\"validation\"])\n",
    "df_test = pd.DataFrame(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text labels       id  \\\n",
      "0      My favourite food is anything I didn't have to...   [27]  eebbqej   \n",
      "1      Now if he does off himself, everyone will thin...   [27]  ed00q6i   \n",
      "2                         WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj   \n",
      "3                            To make her feel threatened   [14]  ed7ypvh   \n",
      "4                                 Dirty Southern Wankers    [3]  ed0bdzj   \n",
      "...                                                  ...    ...      ...   \n",
      "43405  Added you mate well Iâ€™ve just got the bow and ...   [18]  edsb738   \n",
      "43406  Always thought that was funny but is it a refe...    [6]  ee7fdou   \n",
      "43407  What are you talking about? Anything bad that ...    [3]  efgbhks   \n",
      "43408            More like a baptism, with sexy results!   [13]  ed1naf8   \n",
      "43409                                    Enjoy the ride!   [17]  eecwmbq   \n",
      "\n",
      "                                           labels_vector  \n",
      "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
      "4      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                  ...  \n",
      "43405  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "43406  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "43407  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "43408  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
      "43409  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[43410 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert lists into multi-label format\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_train['labels_vector'] = mlb.fit_transform(df_train[\"labels\"]).tolist()\n",
    "\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract label mapping\n",
    "label_names = data['train'].features['labels'].feature.names\n",
    "# print(label_names)  # This will give you something like ['admiration', 'amusement', 'anger', ...]\n",
    "# df_train['labels_text'] = df_train['labels'].apply(lambda x: [label_names[i] for i in x])\n",
    "# print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe file already exists locally.\n",
      "                                                text labels       id  \\\n",
      "0  My favourite food is anything I didn't have to...   [27]  eebbqej   \n",
      "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i   \n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj   \n",
      "3                        To make her feel threatened   [14]  ed7ypvh   \n",
      "4                             Dirty Southern Wankers    [3]  ed0bdzj   \n",
      "\n",
      "                                       labels_vector  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
      "4  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0  my favourite food is anything i didnt have to ...   \n",
      "1  now if he does off himself everyone will think...   \n",
      "2                     why the fuck is bayless isoing   \n",
      "3                        to make her feel threatened   \n",
      "4                             dirty southern wankers   \n",
      "\n",
      "                                       embedded_text  sentence_length  \\\n",
      "0  [-0.25708282, 0.27207917, 0.3068532, -0.274141...               11   \n",
      "1  [0.024942549, 0.28312966, 0.43072635, -0.28969...               20   \n",
      "2  [-0.0674838, -0.14613402, 0.65164256, -0.61750...                6   \n",
      "3  [-0.089369, 0.0204972, 0.3124051, -0.2229408, ...                5   \n",
      "4  [-0.185275, 0.3673805, 0.576995, -0.65662, 0.1...                3   \n",
      "\n",
      "   uppercase_ratio  \n",
      "0         0.033898  \n",
      "1         0.008929  \n",
      "2         0.833333  \n",
      "3         0.037037  \n",
      "4         0.136364  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # Remove emails\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    return \" \".join(tokens)  # Return cleaned text\n",
    "\n",
    "# Load dataset\n",
    "df_train['text'] = df_train['text'].fillna(\"\")  # Handle NaN values\n",
    "df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Define local path for GloVe file\n",
    "glove_file_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "# Check if the file already exists locally\n",
    "if not os.path.exists(glove_file_path):\n",
    "    print(\"GloVe file not found. Downloading from Google Drive...\")\n",
    "    file_id = \"1QsPKoMTyODoqTklndJcbUjVdnGQ804H-\"\n",
    "    try:\n",
    "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", glove_file_path, quiet=False)\n",
    "        print(\"Download completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading: {e}\")\n",
    "        print(\"Please download the file manually from Google Drive\")\n",
    "else:\n",
    "    print(\"GloVe file already exists locally.\")\n",
    "\n",
    "# Load GloVe word embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Convert text to GloVe embeddings\n",
    "def preprocess_for_glove(text, embedding_dict, dim=100):\n",
    "    tokens = text.split()  # Tokenize text (already preprocessed)\n",
    "    embeddings = [embedding_dict[word] for word in tokens if word in embedding_dict]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(dim)\n",
    "\n",
    "df_train['embedded_text'] = df_train['clean_text'].apply(lambda text: preprocess_for_glove(text, glove_embeddings))\n",
    "\n",
    "# Add additional features\n",
    "df_train['sentence_length'] = df_train['clean_text'].apply(lambda x: len(x.split()))\n",
    "df_train['uppercase_ratio'] = df_train['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "Neural Network with 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # Multi-label classification needs Sigmoid activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)  # Sigmoid for multi-label classification\n",
    "\n",
    "\n",
    "# Training with K-Fold Cross-Validation\n",
    "def train_evaluate_model(X_tensor, y_tensor, input_size, hidden_size, num_classes, kf, epochs, learning_rate, criterion, optimizer_class, device):\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_tensor):\n",
    "        X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "        y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "\n",
    "        model = SimpleNN(input_size, hidden_size, num_classes).to(device)\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_acc = ((model(X_train) > 0.5) == y_train).float().mean().item()  # Threshold at 0.5 for multi-label\n",
    "            val_acc = ((model(X_val) > 0.5) == y_val).float().mean().item()\n",
    "            training_accuracies.append(train_acc)\n",
    "            validation_accuracies.append(val_acc)\n",
    "\n",
    "    results = {\n",
    "        'Training Accuracy Mean': np.mean(training_accuracies),\n",
    "        'Training Accuracy StdDev': np.std(training_accuracies),\n",
    "        'Validation Accuracy Mean': np.mean(validation_accuracies),\n",
    "        'Validation Accuracy StdDev': np.std(validation_accuracies)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [27]\n",
      "1    [27]\n",
      "2     [2]\n",
      "3    [14]\n",
      "4     [3]\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train['labels'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Apply preprocessing with word embeddings\n",
    "X_glove = np.array(df_train['embedded_text'].tolist())\n",
    "y_glove = np.array(df_train['labels_vector'].tolist())\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor_glove = torch.tensor(X_glove, dtype=torch.float32)\n",
    "y_tensor_glove = torch.tensor(y_glove, dtype=torch.float32)\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_tensor_glove = X_tensor_glove.to(device)\n",
    "y_tensor_glove = y_tensor_glove.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+--------------------------+----------------------------+\n",
      "| Training Accuracy Mean | Training Accuracy StdDev | Validation Accuracy Mean | Validation Accuracy StdDev |\n",
      "+------------------------+--------------------------+--------------------------+----------------------------+\n",
      "|   0.9533659815788269   |  0.0058698376873344855   |    0.9533369660377502    |    0.005977000497952253    |\n",
      "+------------------------+--------------------------+--------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_size = 100  # GloVe embedding size (100D)\n",
    "hidden_size = 128\n",
    "num_classes = len(label_names)  # Number of emotion labels in GoEmotions dataset\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "kf = KFold(n_splits=5)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Multi-label classification requires BCE loss\n",
    "optimizer_class = optim.Adam\n",
    "\n",
    "# Train the model\n",
    "cv_results = train_evaluate_model(X_tensor_glove, y_tensor_glove, input_size, hidden_size, num_classes, kf, epochs, learning_rate, criterion, optimizer_class, device)\n",
    "\n",
    "# Print Results\n",
    "from tabulate import tabulate\n",
    "print(tabulate([cv_results], headers=\"keys\", tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindscribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
